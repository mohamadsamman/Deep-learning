{"cells":[{"cell_type":"markdown","metadata":{"id":"kLkNJAi9fcfu"},"source":["# Lab 2: Physics Informed Neural Network (PINN)"]},{"cell_type":"markdown","metadata":{"id":"kDL6VKjufcfv"},"source":["## Burger’s equation\n","\n","In one space dimension, the Burger’s equation along with Dirichlet boundary conditions reads as\n","$$\n","\\frac{\\partial u(t,x)}{\\partial t} ~+~ u(t,x) \\frac{\\partial u(t,x)}{\\partial x} ~=~ \\nu \\frac{\\partial^2 u(t,x)}{\\partial^2 x}, x\\in[-1,1], t\\in[0,1],\\\\\n","$$\n","$$\n","u(0,x)=-\\sin(\\pi x),\\\\\n","$$\n","$$\n","u(t,-1)=u(t,1)=0.\n","$$\n","The goal is to compute the function $u(t,x)$ solution of this Partial Derivative Equation (PDE) for $\\nu= \\frac{0.01}{\\pi}$, where $t$ is the time and $x$ the spatial location.\n","\n","Let us define the \"residual function\" $f(t,x)$ which should vanish for all $x$ and $t$:\n","$$\n","f(t,x)=\\frac{\\partial u(t,x)}{\\partial t} ~+~ u(t,x) \\frac{\\partial u(t,x)}{\\partial x} ~-~ \\nu \\frac{\\partial^2 u(t,x)}{\\partial^2 x}.\n","$$\n","\n","The function $u(t,x)$ will be approximated by a neural network.\n","\n","\n","## How to learn the solution?\n","\n","The shared parameters between the neural networks $u(t,x)$ and $f(t,x)$ can be learned by minimizing the mean squared error loss\n","\n","$$\n","MSE = MSE_u + MSE_f\n","$$\n","where\n","$$\n","MSE_u=\\frac{1}{N_u}\\sum_{i=1}^{N_u}{(u(t_u^i,x_u^i)-u^i)}^2\n","$$\n","and\n","$$\n","MSE_f=\\frac{1}{N_f}\\sum_{i=1}^{N_f}{(f(t_f^i,x_f^i))}^2\n","$$\n","\n","Here, $\\{t_u^i,x_u^i,u^i\\}$ denote the initial and boundary training data on $u(t, x)$ and $\\{t_f^i , x_f^i\\}$ specify the collocation points for $f(t, x)$."]},{"cell_type":"markdown","metadata":{"id":"OczeUZovfcfw"},"source":["Import all the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0tFYpPXfcfw"},"outputs":[],"source":["%matplotlib inline\n","\n","import torch\n","import numpy as np\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from torch import optim\n","from torch.autograd import grad"]},{"cell_type":"markdown","metadata":{"id":"IWR33dt_fcfw"},"source":["## Numerical solution\n","\n","The following cell use the function \"burgers_viscous_time_exact1\" that gives a very accurate numerical solution. This solution does not rely on deep neural networks. It will be the reference solution for our deep neural network based solution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoqGOViTfcfw"},"outputs":[],"source":["from burgersnumlib import burgers_viscous_time_exact1\n","\n","# Define the discretization grid and the parameters of the Burger equation\n","vtn = 40 # number of time samples\n","vxn = 20 # number of space samples\n","nu = 0.01 / np.pi\n","\n","# Space discretization\n","xlo = -1.0\n","xhi = +1.0\n","vx = np.linspace ( xlo, xhi, vxn )\n","\n","# Time discretization\n","tlo = 0.0\n","thi = 1.0\n","vt = np.linspace ( tlo, thi, vtn )\n","\n","# Compute the solution\n","# Each column of \"u_true\" corresponds to a given time\n","u_true = burgers_viscous_time_exact1 ( nu, vxn, vx, vtn, vt )\n","u_true = np.transpose(u_true) # we transpose the result because the burger simulation returns the function u(x,t) instead of u(t,x)\n","\n","# Check the size of the solution\n","print(u_true.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-G5lTuWLfcfx"},"outputs":[],"source":["# Transform the input into a list of vectors\n","def flat(x):\n","    m = x.shape[0]\n","    return [x[i] for i in range(m)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S_rz8qufcfx"},"outputs":[],"source":["# Example for \"flat\"\n","x = torch.arange(0, 4, 1.0).reshape(2, 2)\n","print(x)\n","print(flat(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXr7upkPfcfx"},"outputs":[],"source":["# This function computes the n-th order derivative of a function f\n","# f: the function\n","# x: inputs\n","# n: the order of derivation\n","\n","# FYI (For Your Information):\n","# create_graph (bool, optional) – If True, graph of the derivative will be constructed,\n","# allowing to compute higher order derivative products. Default: False.\n","\n","def nth_derivative(f, x, n):\n","    for i in range(n):\n","        grads = grad(f, x, create_graph=True, allow_unused=True)[0]\n","        f = grads\n","        if grads is None:\n","            print('bad grad')\n","            return torch.tensor(0.)\n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHj5EWnvfcfx"},"outputs":[],"source":["# Example for \"nth_derivative\"\n","x = torch.arange(0, 4, 1.0, requires_grad=True).reshape(2, 2)\n","loss = (x ** 4).sum()\n","\n","print(x)\n","print(nth_derivative(f=loss, x=x, n=1)) # Gradient calculated numerically\n","print(4*x**3) # Gradient calculated by hand\n"]},{"cell_type":"markdown","metadata":{"id":"yIGBcVTyfcfx"},"source":["## Define the Neural Network class.\n","\n","We want to implement the following neural network:\n","\n","Input -> Linear -> Tanh -> Hidden 1\n","\n","-> Linear -> Tanh -> Hidden 2\n","\n","-> Linear -> Tanh -> Hidden 3\n","\n","-> Linear -> Tanh -> Hidden 4\n","\n","-> Linear -> Output"]},{"cell_type":"markdown","metadata":{"id":"7R9tAGrvfcfx"},"source":["The neural network will be a function:\n","\n","$$\n","net(t,x):~ [0, 1] \\times [-1, 1]~  \\mapsto ~ \\mathbb{R}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"UrUkxig_fcfx"},"source":["### Question: what is the input of the neural network during the training?"]},{"cell_type":"markdown","metadata":{"id":"q4OtG0QEfcfy"},"source":["Explanation:...\n","\n","Brief answer: We have several couples $(x_i,t_i)$ and one neural network output for each couple $(x_i,t_i)$."]},{"cell_type":"markdown","metadata":{"id":"0cXv5fB7fcfy"},"source":["### Question: Code the neural network in the following \"forward\" function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oFmie2hfcfy"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.il  = nn.Linear(2,80)\n","        self.hl1 = nn.Linear(80,80)\n","        self.hl2 = nn.Linear(80,80)\n","        self.hl3 = nn.Linear(80,40)\n","        self.ol  = nn.Linear(40,1)\n","        self.tn  = nn.Tanh()\n","\n","    def forward(self,t,x):\n","        u = torch.cat((t, x), 1)\n","        hidden1 = self.tn(self.il(u))\n","        hidden2 = self.tn(self.hl1(hidden1))\n","        hidden3 = self.tn(self.hl2(hidden2))\n","        hidden4 = self.tn(self.hl3(hidden3))\n","        out     = self.ol(hidden4)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxIrgeN_fcfy"},"outputs":[],"source":["def f(t,x):\n","    u = mynet(t,x)\n","    u_t = nth_derivative(flat(u), x=t, n=1)\n","    u_x = nth_derivative(flat(u), x=x, n=1)\n","    u_xx = nth_derivative(flat(u_x), x=x, n=1)\n","    eta = torch.tensor(0.01/np.pi)\n","    f = u_t + u*u_x - eta*u_xx\n","    return f"]},{"cell_type":"markdown","metadata":{"id":"-t4fS6Gbfcfy"},"source":["### Initial conditions and boundaries of the PDE solution:\n","\n","$$\n","u(0,x)=-\\sin(\\pi x),\\\\\n","$$\n","$$\n","u(t,-1)=u(t,1)=0.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"ZlzUsZNkfcfy"},"source":["Conversion into Pytorch tensors and set up the gradient status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAMOipICfcfy"},"outputs":[],"source":["# ivx is the set of x axis points as a tensor\n","ivx = torch.from_numpy(vx).float()\n","ivx =  ivx.reshape(vxn,1)\n","ivx.requires_grad = True\n","\n","# ivt is the set of t axis points as a tensor\n","ivt = torch.from_numpy(vt).float()\n","ivt = ivt.reshape(vtn,1)\n","ivt.requires_grad = True"]},{"cell_type":"markdown","metadata":{"id":"KHvnjXGsfcfy"},"source":["## Define the first training set for the boundary conditions"]},{"cell_type":"markdown","metadata":{"id":"QLC-Py8_fcfy"},"source":["### Question: how to interpret the set \"full_u\"?"]},{"cell_type":"markdown","metadata":{"id":"shAaHGLPfcfy"},"source":["Brief answer: it contains all the boundary conditions necessary to learn the appropriate neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G14b93wqfcfy"},"outputs":[],"source":["# we need to create the boundary conditions.\n","# There are three segments when x = -1, x = 1 and t=0\n","# it0 is t=0 for all x values (hence, it is a zero tensor of size \"vxn\")\n","# ix1 is x=1 for all t (hence, it is a one tensor of size \"vtn\")\n","# ixm1 is x=-1 for all t (hence, it is a minus one tensor of size \"vtn\")\n","it0 = torch.zeros(vxn, dtype=torch.float, requires_grad=True).reshape(vxn,1)\n","ix1 = torch.zeros(vtn, dtype=torch.float, requires_grad=True).reshape(vtn,1)+1.0\n","ixm1  = torch.zeros(vtn, dtype=torch.float, requires_grad=True).reshape(vtn,1)-1.0\n","\n","# Now the values for u on the above boundary segments\n","# We use the numerical solution to get the boundary conditions\n","xbndr = torch.from_numpy(-np.sin(np.pi*vx)).float().reshape(vxn,1) # boundary u(0,x) at t = 0\n","\n","# Generate the tensors containing the triplets (t,x,u(t,x)) for the boundary conditions\n","# Each tensor is obtained by concatenating horizontally the 3 basic tensors of a specific condition\n","# There are 3 \"full\" tensors because there are 3 conditions\n","full_it0  = torch.cat((it0, ivx, xbndr),dim=1) # get all the triplets (0,x,u(0,x))\n","full_ixm1 = torch.cat((ivt, ixm1, torch.zeros(vtn,1)),dim=1) # get all the triplets (t,-1, u(t,-1))\n","full_ix1  = torch.cat((ivt, ix1, torch.zeros(vtn,1)),dim=1) # get all the triplets (t,1, u(t,1))\n","\n","# Concatenate all the boundary condition tensors\n","full_u = torch.cat((full_it0, full_ixm1, full_ix1), dim=0) # concatenate vertically the 3 tensors"]},{"cell_type":"markdown","metadata":{"id":"b6qrt18ifcfy"},"source":["## Define the second training set for $f(t,x)$"]},{"cell_type":"markdown","metadata":{"id":"ikDbhtS4fcfy"},"source":["### Question: how to interpret \"full_f\"?"]},{"cell_type":"markdown","metadata":{"id":"CBC6KNSHfcfy"},"source":["Brief answer: \"full_f\" is a list of all triplets $(x, t, 0)$ for all $x$ and $t$\n","so it describes the interior of the definition domain of $f(t,x)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIMO4z2mfcfy"},"outputs":[],"source":["grid_ivt, grid_ivx = torch.meshgrid(ivt.squeeze(), ivx.squeeze(),indexing=\"ij\") # sampling the 2D domain of f(t,x)\n","grid_ivt_1D = grid_ivt.reshape([-1,1]) # convert into 1D vector\n","grid_ivx_1D = grid_ivx.reshape([-1,1]) # convert into 1D vector\n","zeros = torch.zeros(grid_ivt_1D.size(), dtype=torch.float, requires_grad=True) # create a vector filled in with 0\n","full_f = torch.cat((grid_ivt_1D, grid_ivx_1D, zeros),dim=1) # concatenate the 3 tensors"]},{"cell_type":"markdown","metadata":{"id":"prqe4zN0fcfy"},"source":["### Just a check\n","\n","Run the neural network to check its behavior (with random weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbRO_T7_fcfy"},"outputs":[],"source":["mynet =  Net()\n","u = mynet(it0,ivx) # compute u(0,x) for all x\n","print(u.shape)"]},{"cell_type":"markdown","metadata":{"id":"vjmsNWXJfcfy"},"source":["## Train the neural network"]},{"cell_type":"markdown","metadata":{"id":"0eW19LFcfcfy"},"source":["### Question: what is an epoch?"]},{"cell_type":"markdown","metadata":{"id":"n6EFPnKDfcfy"},"source":["Brief answer: an epoch corresponds to a training iteration when the the full training dataset is processed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP6Cupn6fcfy"},"outputs":[],"source":["# number of epochs\n","epochs = 200 # 4000 epochs is better but longer!\n","\n","# Define the learning rate\n","learning_rate = 0.001"]},{"cell_type":"markdown","metadata":{"id":"kmUKaeZkfcfz"},"source":["Initialization of the train step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rbv5XaQKfcfz"},"outputs":[],"source":["# instantiate the neural network\n","mynet = Net()\n","\n","# Set the loss function\n","loss_fn = nn.MSELoss()\n","\n","# Use Adam optimizer\n","optimizer = optim.Adam(mynet.parameters(), lr=learning_rate)\n"]},{"cell_type":"markdown","metadata":{"id":"GLdgSEMZfcfz"},"source":["### Question: why do we set \"retain_graph=True\"?"]},{"cell_type":"markdown","metadata":{"id":"itOQwMiCfcfz"},"source":["Explanation:...\n","\n","Brief answer: Because we need to compute second-order derivatives."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZcgedgGfcfz"},"outputs":[],"source":["for epoc in range(1, epochs+1):\n","    # Before the backward pass, use the optimizer object to zero all of the\n","    # gradients for the variables it will update (which are the learnable\n","    # weights of the model). This is because by default, gradients are\n","    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n","    # is called. Checkout docs of torch.autograd.backward for more details.\n","    optimizer.zero_grad()\n","\n","    # Compute the outputs of the network\n","    outputs_u = mynet(torch.narrow(full_u,1,0,1), torch.narrow(full_u,1,1,1))\n","    outputs_f = f(torch.narrow(full_f,1,0,1), torch.narrow(full_f,1,1,1)) # narrow extracts a column\n","\n","    # Compute the two losses:\n","    # 1- We must minimize the function f(x,t)\n","    # 2- And we must also check the boundary constraints\n","    mse_u = loss_fn(outputs_u, torch.narrow(full_u,1,2,1))\n","    mse_f = loss_fn(outputs_f, torch.narrow(full_f,1,2,1) )\n","\n","    # We want to minimize the sum of losses\n","    mse = mse_u + mse_f\n","\n","    # Backward pass: compute gradient of the loss with respect to model parameters\n","    mse.backward(retain_graph=True)\n","\n","    # Calling the step function on an Optimizer makes an update to its\n","    # parameters\n","    optimizer.step()\n","\n","    if epoc % 200 == 0:\n","        # print the epoc information on screen\n","        print('epoc %d; boundary loss %f, function loss %f, sum loss %f'\n","              %(epoc, float(mse_u), float(mse_f), float(mse) ) )"]},{"cell_type":"markdown","metadata":{"id":"40QMIkpOfcf2"},"source":["### Plot the solution"]},{"cell_type":"markdown","metadata":{"id":"JolicsKofcf2"},"source":["Plot the solution at time t=0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FsE2ZJ4fcf2"},"outputs":[],"source":["u = mynet(it0,ivx)\n","uu = u.detach().numpy()\n","print(uu.shape)\n","plt.plot(vx, u_true[0,:],'k--',label='True solution')\n","plt.plot(vx, -np.sin(np.pi*vx),'rx',label='Sine function')\n","plt.plot(vx, u.detach().numpy(),'b-o',label='DNN solution')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fWUgbiITfcf2"},"source":["Plot the function $u(t,x)$ as a function of $x$ at four specific times"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAIXu5qnfcf2"},"outputs":[],"source":["# Compute the solution for all couples (t,x)\n","u = mynet(grid_ivt_1D, grid_ivx_1D).reshape(vtn,vxn).detach().numpy()\n","print(u.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AQ7h3f1fcf2"},"outputs":[],"source":["fig, axs = plt.subplots(2, 2)\n","axs[0, 0].plot(vx, u[0,:])\n","axs[0, 0].plot(vx, u_true[0,:],'k--')\n","axs[0, 0].set_title('t=0')\n","\n","axs[0, 1].set_title('t=0.25')\n","axs[0, 1].plot(vx, u[int(vtn/4),:],  'tab:orange')\n","axs[0, 1].plot(vx, u_true[int(vtn/4),:],'k--')\n","\n","axs[1, 0].set_title('t=0.5')\n","axs[1, 0].plot(vx, u[int(vtn/2),:],  'tab:green')\n","axs[1, 0].plot(vx, u_true[int(vtn/2),:],'k--')\n","\n","axs[1, 1].set_title('t=0.99')\n","axs[1, 1].plot(vx, u[vtn-1,:], 'tab:red')\n","axs[1, 1].plot(vx, u_true[vtn-1,:],'k--')\n","\n","for ax in axs.flat:\n","    ax.set(xlabel='x axis', ylabel='u value')\n","for ax in axs.flat:\n","    ax.label_outer()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}